# app.py
import os
import pickle
import textwrap
import streamlit as st
import models.database as db

from langchain_core.messages.chat import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable, RunnablePassthrough
from langchain_openai import ChatOpenAI
from initialize import init_rag
from services.search import search_top_k

# ê°„ë‹¨í•œ ë¡œê¹…ì€ ê¸°ë³¸ print()ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜, í•„ìš”ì‹œ ë‹¤ë¥¸ ë¡œê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©
print("[Project] Yong-in RAG")

# â˜… ìˆ˜ì •ëœ íŒŒì¼ ê²½ë¡œ â˜…
INDEX_FILE = "rag_index/index.faiss"
CHUNKED_FILE = "rag_index/index.pkl"
DATA_DIR = "crawling/output"


GREETING_MESSAGE = textwrap.dedent(
    """\
ì•ˆë…•í•˜ì„¸ìš”! ë” ë‚˜ì€ ì‚¶ì„ ìœ„í•œ **ìŠ¤ë§ˆíŠ¸ë„ì‹œ**, ìš©ì¸ì‹œì²­ ì±—ë´‡ì…ë‹ˆë‹¤.  

ì €ëŠ” **ì¡°ì§ë„ ì •ë³´**ë¥¼ ì‹¤ì‹œê°„ ì•ˆë‚´í•´ ë“œë¦¬ê³  ìˆì–´ìš”.  

ğŸ“Œ TIP! ì´ë ‡ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”!


  - ë¯¼ì› ë‹´ë‹¹ì ì—°ë½ì²˜ ì•Œë ¤ì¤˜
  - ì²­ë…„ ì›”ì„¸ì§€ì›ë‹´ë‹¹ì ì—°ë½ì²˜ ì•Œë ¤ì¤˜
  - ì²­ë…„ ì·¨ì—…ì§€ì› í•´ì£¼ëŠ” ë‹´ë‹¹ì ì•Œë ¤ì¤˜
"""
)



# --- ì¶”ê°€: ëŒ€í™” ë‚´ì—­ ìš”ì•½ í•¨ìˆ˜ ---
def summarize_conversation(history_text: str) -> str:
    """
    ì£¼ì–´ì§„ ëŒ€í™” ë‚´ì—­ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    llm_summary = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=False)
    summary_prompt = (
        f"ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜:\n\n{history_text}\n\nê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜."
    )
    summary_response = llm_summary.invoke(summary_prompt)
    if hasattr(summary_response, "content"):
        summary_text = summary_response.content
    else:
        summary_text = str(summary_response)
    return summary_text.strip()


##############################
## FAISS ì¸ë±ìŠ¤ ìë™ ìƒì„± ë° ë¡œë“œ
##############################

def load_or_create_index():
    if os.path.exists(INDEX_FILE):
        # ë¬¸ì„œ ë°ì´í„° ë¡œë“œ
        db.load_data(DATA_DIR)

        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ìƒˆ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©)
        db.load_index(INDEX_FILE, index_type="FLAT")

        # chunked_data ë¡œë“œ (ì—†ìœ¼ë©´ ê²½ê³ )
        try:
            with open(CHUNKED_FILE, "rb") as f:
                loaded_chunked = pickle.load(f)
            # loaded_chunkedê°€ tupleì´ë©´ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if isinstance(loaded_chunked, tuple):
                db.chunked_data = loaded_chunked[0]
            else:
                db.chunked_data = loaded_chunked
        except FileNotFoundError:
            st.warning("âš ï¸ `chunked_data.pkl` íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ `chunked_data.pkl` ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.write("ğŸ”„ FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„± ì¤‘...")
        init_rag(
            data_dir=DATA_DIR,
            chunk_strategy="recursive",
            chunk_param=500,
            index_type="FLAT",
            output_index_path=INDEX_FILE,
            output_chunk_path=CHUNKED_FILE,
        )
        st.success("âœ… ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        db.load_data(DATA_DIR)
        db.load_index(INDEX_FILE, index_type="FLAT")
        try:
            with open(CHUNKED_FILE, "rb") as f:
                loaded_chunked = pickle.load(f)
            if isinstance(loaded_chunked, tuple):
                db.chunked_data = loaded_chunked[0]
            else:
                db.chunked_data = loaded_chunked
            st.success("âœ… ì¸ë±ìŠ¤ ë° chunked_data ë¡œë“œ ì™„ë£Œ!")
        except FileNotFoundError:
            st.warning("âš ï¸ `chunked_data.pkl` íŒŒì¼ì´ ì—¬ì „íˆ ì—†ìŠµë‹ˆë‹¤.")


load_or_create_index()

##############################
## íƒ€ì´í‹€ ë° ì¸ì‚¬ë§ ì¶”ê°€
##############################
st.title("ìš©ì¸ ì‹œì²­ RAG ì±—ë´‡")


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ ë° ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


def is_greeting(text: str) -> bool:
    """
    ì¸ì‚¬ë§ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜.
    ê°„ë‹¨í•œ ì¸ì‚¬ë§ì€ í™˜ì˜ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    greetings = ["ì•ˆë…•", "ì•ˆë…•?", "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì„¸ìš”?"]
    return text.strip() in greetings


# --- ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ runnable ì •ì˜ ---
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼(prompts/yongin.yml)ì„ ì½ì–´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì±„ì›Œ ë„£ëŠ” ì—­í• 
def load_prompt(file_path: str, encoding: str = "utf-8") -> str:
    with open(file_path, encoding=encoding) as f:
        return f.read()

class RunnablePrompt(Runnable):
    def __init__(self, prompt_template: str):
        self.prompt_template = prompt_template

    def invoke(self, input_dict: dict, config=None, **kwargs) -> str:
        # input_dictì—ì„œ "question" í‚¤ë¥¼ ê°€ì ¸ì™€ í…œí”Œë¦¿ì— ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.
        question = input_dict.get("question", "")
        prompt_text = self.prompt_template.format(question=question)
        # ì´ì œ ë¬¸ìì—´(prompt_text)ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        return prompt_text

    # (ì¶”ê°€ ë©”ì„œë“œ êµ¬í˜„ì€ í•„ìš”ì— ë”°ë¼)

# --- ì²´ì¸ ìƒì„± ---
def create_chain(model_name="gpt-4o-mini"):
    # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ yml í™•ì¥ìë¡œ ë³€ê²½í•˜ì—¬ ë¡œë“œ
    prompt_template = load_prompt("prompts/yongin.yaml", encoding="utf-8")
    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ runnable ìƒì„±
    prompt_runnable = RunnablePrompt(prompt_template)
    # streaming=True ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í™œì„±í™”í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name=model_name, temperature=0, streaming=True)
    # ì²´ì¸ êµ¬ì„±: ì´ˆê¸° ì…ë ¥ì€ {"question": <ì‚¬ìš©ì ì§ˆë¬¸>}ë¥¼ RunnablePassthrough()ë¡œ ê·¸ëŒ€ë¡œ ë„˜ê¸°ê³ ,
    # ê·¸ ë‹¤ìŒ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ runnableë¡œ í…œí”Œë¦¿ ì ìš©, ì´í›„ llm í˜¸ì¶œ, ë§ˆì§€ë§‰ì— StrOutputParser()ë¡œ ê²°ê³¼ íŒŒì‹±.
    chain = {"question": RunnablePassthrough()} | prompt_runnable | llm | StrOutputParser()
    return chain


# ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„±
def rewrite_query(user_question: str) -> str:
    """
    LLMì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìš©ì¸ì‹œì²­ ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    """
    llm_rewriter = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=False)
    rewriter_prompt = (
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìš©ì¸ì‹œì²­ì— ê´€ë ¨ëœ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ì–´ì¤˜. "
        "ì§ˆë¬¸ì˜ ì˜ë¯¸ì™€ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•´ì„œ ê²€ìƒ‰ ê²°ê³¼ì— ìµœì‹  ì†Œì‹ì´ ì˜ í¬í•¨ë  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•´ì¤˜.\n"
        f"ì§ˆë¬¸: {user_question}\n"
        "ê²€ìƒ‰ ì¿¼ë¦¬:"
    )
    rewritten = llm_rewriter.invoke(rewriter_prompt)
    if hasattr(rewritten, "content"):
        return rewritten.content.strip()
    else:
        return str(rewritten).strip()


# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    chain = create_chain(model_name="gpt-4o-mini")
    st.session_state["chain"] = chain

## ìµœì´ˆ ì ‘ì† ì‹œ ì±—ë´‡ ì¸ì‚¬ë§ ìë™ ì¶”ê°€ (ëŒ€í™”ê°€ ì‹œì‘ë˜ì§€ ì•Šì€ ê²½ìš°)
if not st.session_state["messages"]:
    add_message("assistant", GREETING_MESSAGE)

# ì‚¬ì´ë“œë°”: ì´ˆê¸°í™” ë²„íŠ¼ê³¼ ëª¨ë¸ ì„ íƒ ë©”ë‰´ (ì£¼ì„ ì²˜ë¦¬ëœ ë¶€ë¶„ì€ í•„ìš”ì— ë”°ë¼ í™œì„±í™”)
selected_model = "gpt-4o-mini"

print_messages()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ì±— ì…ë ¥)
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning_msg = st.empty()

if user_input:
    chain = st.session_state["chain"]
    if chain is not None:
        st.chat_message("user").write(user_input)

        # ë¨¼ì € ì‚¬ìš©ì ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        query_for_search = user_input
        results = search_top_k(query_for_search, top_k=5, ranking_mode="rrf")
        print("RESULT: ", results)
        if not results or len(results) == 0:
            with st.spinner("ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                query_for_search = rewrite_query(user_input)
            results = search_top_k(query_for_search, top_k=5, ranking_mode="rrf")
        if not results or len(results) == 0:
            context_text = "âŒ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
            answer_chunks = []
            for r in results[:3]:
                chunk_text = r.get("chunk_text", "ë‚´ìš© ì—†ìŒ")
                doc_url = r.get("original_doc", {}).get("url", "ì¶œì²˜ ì—†ìŒ")
                enriched_chunk = f"ì´ chunkëŠ” {doc_url} ì—ì„œ ê°€ì ¸ì˜¨ ë‚´ìš©ì…ë‹ˆë‹¤.\n{chunk_text}"
                answer_chunks.append(enriched_chunk)
            context_text = "\n\n".join(answer_chunks)
        # --- RAG: end ---

        conversation_history = ""
        if st.session_state["messages"]:
            recent_msgs = st.session_state["messages"][-5:]
            for msg in recent_msgs:
                conversation_history += f"{msg.role.capitalize()}: {msg.content}\n"
            if len(conversation_history) > 500:
                conversation_history = summarize_conversation(conversation_history)
        conversation_section = ""
        if conversation_history:
            conversation_section = f"ì´ì „ ëŒ€í™” ë‚´ìš©:\n{conversation_history}\n"
        combined_query = (
            f"ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© (RAG):\n{context_text}\n\n"
            f"{conversation_section}"
            f"ìµœì¢… ì§ˆë¬¸: {user_input}"
        )

        response_generator = chain.stream(combined_query)
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            spinner_placeholder = st.empty()
            spinner_placeholder.markdown("**ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...**")
            for token in response_generator:
                if ai_answer == "":
                    spinner_placeholder.empty()
                ai_answer += token
                container.markdown(ai_answer)

        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        warning_msg.error("ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")