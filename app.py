import os
import pickle

import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.messages.chat import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_teddynote import logging
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter

import models.database as db
from initialize import init_rag

# âœ… FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
INDEX_FILE = "faiss_index.bin"
CHUNKED_FILE = "chunked_data.pkl"
DATA_DIR = "data/yongin_data2"


# FAISS ì¸ë±ìŠ¤ ìë™ ìƒì„± ë° ë¡œë“œ
def load_or_create_index():
    if os.path.exists(INDEX_FILE):
        # ë¬¸ì„œ ë°ì´í„° ë¡œë“œ
        db.load_data(DATA_DIR)

        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        db.load_index(INDEX_FILE, index_type="FLAT")

        # chunked_data ë¡œë“œ (ì—†ìœ¼ë©´ ê²½ê³ )
        try:
            with open(CHUNKED_FILE, "rb") as f:
                db.chunked_data = pickle.load(f)
        except FileNotFoundError:
            st.warning(
                "âš ï¸ `chunked_data.pkl` íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        except Exception as e:
            st.error(f"âŒ `chunked_data.pkl` ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.write("ğŸ”„ FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„± ì¤‘...")
        init_rag(
            data_dir=DATA_DIR,
            chunk_strategy="recursive",
            chunk_param=500,
            index_type="FLAT",
            output_index_path=INDEX_FILE,
            output_chunk_path=CHUNKED_FILE,
        )
        st.success("âœ… ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        db.load_data(DATA_DIR)
        db.load_index(INDEX_FILE, index_type="FLAT")
        try:
            with open(CHUNKED_FILE, "rb") as f:
                db.chunked_data = pickle.load(f)
            st.success("âœ… ì¸ë±ìŠ¤ ë° chunked_data ë¡œë“œ ì™„ë£Œ!")
        except FileNotFoundError:
            st.warning("âš ï¸ `chunked_data.pkl` íŒŒì¼ì´ ì—¬ì „íˆ ì—†ìŠµë‹ˆë‹¤.")


load_or_create_index()

##############################
## íƒ€ì´í‹€ ë° ì¸ì‚¬ë§ ì¶”ê°€
##############################
st.title("ìš©ì¸ ì‹œì²­ RAG ì±—ë´‡")
st.write(
    "ì•ˆë…•í•˜ì„¸ìš”! ìš©ì¸ì‹œ ê´€ë ¨ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ì•„ë˜ ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."
)


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ ë° ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ì²´ì¸ ìƒì„± (LangChainì˜ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ ì ìš©)
def create_chain(model_name="gpt-4o"):
    prompt = load_prompt("prompts/yongin.yaml", encoding="utf-8")
    # streaming=True ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í™œì„±í™”í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name=model_name, temperature=0, streaming=True)
    chain = {"question": RunnablePassthrough()} | prompt | llm | StrOutputParser()
    return chain


# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    chain = create_chain(model_name="gpt-4o")
    st.session_state["chain"] = chain

# ì‚¬ì´ë“œë°”: ì´ˆê¸°í™” ë²„íŠ¼ê³¼ ëª¨ë¸ ì„ íƒ ë©”ë‰´
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    selected_model = st.selectbox(
        "LLM ì„ íƒ", ["gpt-4o-mini", "gpt-4-turbo", "gpt-4o"], index=0
    )

if clear_btn:
    st.session_state["messages"] = []

print_messages()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ì±— ì…ë ¥)
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning_msg = st.empty()

if user_input:
    chain = st.session_state["chain"]
    if chain is not None:
        st.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì±—ë´‡ ì‘ë‹µ ìƒì„±
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            # for ë£¨í”„ë¥¼ í†µí•´ í† í° ë‹¨ìœ„ë¡œ ì¶œë ¥
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)
        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        warning_msg.error("ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
