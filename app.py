# app.py
import os
import pickle

import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable, RunnablePassthrough
from langchain_openai import ChatOpenAI

import models.database as db
from services.initialize import init_rag
from services.load_or_create_index import load_or_create_index
from services.search import search_top_k
from utils.chat import (
    add_message,
    create_chain,
    get_context_text,
    print_messages,
    rewrite_query,
    summarize_sources,
)
from utils.custom_logging import langsmith
from utils.greeting_message import GREETING_MESSAGE
from utils.logging import log_debug

langsmith(project_name="Yong-in RAG")

# ì±„íŒ… ì…ë ¥ì°½ ë†’ì´ ì¡°ì •ì„ ìœ„í•œ CSS ì¶”ê°€
st.markdown(
    """
<style>
.stMain {
    position: relative;
}
.stChatMessage {
    background-color: transparent !important;
}
[data-testid=stSidebar] {
    background-color: #3d9df3;
    padding:0 15px;
}

[data-testid=stSidebarUserContent] {
    background-color: white;
    border-radius: 10px;
}

/* ì±„íŒ… ì…ë ¥ì°½ ë†’ì´ ì¡°ì • - ìƒˆë¡œìš´ í´ë˜ìŠ¤ëª… ì‚¬ìš© */
.st-emotion-cache-glsyku {
    min-height: 80px !important;
    align-items: center;
}
.st-emotion-cache-glsyku textarea:active{
    outline: none;
}
.st-emotion-cache-glsyku div {
    display: flex;
    align-items: center;
    justify-content: center;
}

[data-testid="stChatMessage"]:has(> [data-testid="stChatMessageAvatarUser"]) {
    justify-content: flex-end !important;
    display: flex !important;
}
[data-testid="stChatMessage"]:has(> [data-testid="stChatMessageAvatarUser"]) [data-testid="stChatMessageContent"] {
    text-align: right !important;
    background-color: #3399FF !important;
    color: #FFFFFF !important;
    border-radius: 10px !important;
    padding: 10px !important;
    margin: 5px 0 !important;
    max-width: 80% !important;
    flex-grow: 0 !important;
}
[data-baseweb="textarea"] {
    border-color: transparent !important;
}
/* ì±„íŒ… ì…ë ¥ì°½ ë†’ì´ ì¡°ì • */
/*
.st-emotion-cache-qcqlej {
    height: 0 !important;
    flex-grow: 0 !important;
}
*/
""",
    unsafe_allow_html=True,
)

# RAG ëª¨ë“œ ì„¤ì •
RAG_MODES = {
    "base": {
        "name": "ê¸°ë³¸ ëª¨ë“œ",
        "description": "í™ˆí˜ì´ì§€ ê¸°ë°˜ êµ¬ì • ì •ë³´, ì£¼ìš” í–‰ì‚¬ ë“±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.",
        "index_file": "faiss_index.bin",
        "chunked_file": "chunked_data.pkl",
        "data_dir": "data/yongin_data2",
        "prompt_file": "prompts/yongin_base.yaml",
    },
    "contact": {
        "name": "ì¡°ì§ë„ ëª¨ë“œ",
        "description": "ì¡°ì§ë„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.",
        "index_file": "rag_index/index.faiss",
        "chunked_file": "rag_index/index.pkl",
        "data_dir": "crawling/output",
        "prompt_file": "prompts/yongin_contact.yaml",
    },
    "article": {
        "name": "ê¸°ì‚¬ ì‘ì„± ëª¨ë“œ",
        "description": "ê¸°ì‚¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.",
        "prompt_file": "prompts/yongin_article.yaml",
    },
    "research": {  # ğŸ†• ì´ë ¥ì„œ ì‘ì„± ëª¨ë“œ ì¶”ê°€
        "name": "ì—°êµ¬ê³¼ì œì‘ì„± ëª¨ë“œ",
        "description": "ì—°êµ¬ê³¼ì œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.",
        "prompt_file": "prompts/yongin_research.yaml",
    },
    "policy": {  # ğŸ†• ì´ë©”ì¼ ì‘ì„± ëª¨ë“œ ì¶”ê°€
        "name": "ì •ì±…ë³´ê³ ì„œ ëª¨ë“œ",
        "description": "ì •ì±…ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.",
        "prompt_file": "prompts/yongin_policy.yaml",
    },
    "event_doc": {  # ğŸ†• ì œì•ˆì„œ ì‘ì„± ëª¨ë“œ ì¶”ê°€
        "name": "í–‰ì‚¬ë³´ê³ ì„œ ëª¨ë“œ",
        "description": "í–‰ì‚¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.",
        "prompt_file": "prompts/yongin_event_doc.yaml",
    },
}

# ì „ì—­ ë³€ìˆ˜ ì œê±° (ëª¨ë“œë³„ ì„¤ì •ìœ¼ë¡œ ëŒ€ì²´)
# INDEX_FILE = "rag_index/index.faiss"
# CHUNKED_FILE = "rag_index/index.pkl"
# DATA_DIR = "crawling/output"


# --- ì¶”ê°€: ëŒ€í™” ë‚´ì—­ ìš”ì•½ í•¨ìˆ˜ ---
def summarize_conversation(history_text: str) -> str:
    """
    ì£¼ì–´ì§„ ëŒ€í™” ë‚´ì—­ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    llm_summary = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=False)
    summary_prompt = (
        f"ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜:\n\n{history_text}\n\nê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜."
    )
    summary_response = llm_summary.invoke(summary_prompt)
    if hasattr(summary_response, "content"):
        summary_text = summary_response.content
    else:
        summary_text = str(summary_response)
    return summary_text.strip()


##############################
## FAISS ì¸ë±ìŠ¤ ìë™ ìƒì„± ë° ë¡œë“œ
##############################


def reset_db_state():
    """
    ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    ëª¨ë“œ ì „í™˜ ì‹œ ì´ì „ ëª¨ë“œì˜ ë°ì´í„°ê°€ ë‚¨ì•„ìˆëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    # ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”
    db.documents = []
    db.chunked_data = {}
    db.index = None


def load_or_create_index(mode="base"):
    """
    ì„ íƒëœ ëª¨ë“œì— ë”°ë¼ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì´ˆê¸°í™”
    reset_db_state()

    mode_config = RAG_MODES[mode]

    # ğŸ“Œ "doc" ëª¨ë“œëŠ” RAGë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•  í•„ìš” ì—†ìŒ
    if mode in ["article", "research", "policy", "event_doc"]:
        log_debug("ğŸ“Œ 'doc' ëª¨ë“œì—ì„œëŠ” FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ì¼ë°˜ì ì¸ RAG ëª¨ë“œ ì²˜ë¦¬
    INDEX_FILE = mode_config.get("index_file", None)
    CHUNKED_FILE = mode_config.get("chunked_file", None)
    DATA_DIR = mode_config.get("data_dir", None)

    if not INDEX_FILE or not CHUNKED_FILE or not DATA_DIR:
        log_debug(f"âŒ {mode} ëª¨ë“œì—ì„œ í•„ìš”í•œ íŒŒì¼ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    if os.path.exists(INDEX_FILE):
        db.load_data(DATA_DIR)
        db.load_index(INDEX_FILE, index_type="FLAT")

        try:
            with open(CHUNKED_FILE, "rb") as f:
                loaded_chunked = pickle.load(f)
            db.chunked_data = (
                loaded_chunked[0]
                if isinstance(loaded_chunked, tuple)
                else loaded_chunked
            )

            log_debug(f"ë¬¸ì„œ ê°œìˆ˜: {len(db.documents)}")
            log_debug(f"ì²­í¬ ê°œìˆ˜: {len(db.chunked_data.get('all_chunks', []))}")
            log_debug(f"ì¸ë±ìŠ¤ í¬ê¸°: {db.index.ntotal if db.index else 0}")

        except FileNotFoundError:
            st.warning(f"âš ï¸ `{CHUNKED_FILE}` íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ `{CHUNKED_FILE}` ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    else:
        st.write(f"ğŸ”„ FAISS ì¸ë±ìŠ¤({INDEX_FILE})ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„± ì¤‘...")
        init_rag(
            data_dir=DATA_DIR,
            chunk_strategy="recursive",
            chunk_param=500,
            index_type="FLAT",
            output_index_path=INDEX_FILE,
            output_chunk_path=CHUNKED_FILE,
        )
        st.success("âœ… ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        db.load_data(DATA_DIR)
        db.load_index(INDEX_FILE, index_type="FLAT")

        try:
            with open(CHUNKED_FILE, "rb") as f:
                loaded_chunked = pickle.load(f)
            db.chunked_data = (
                loaded_chunked[0]
                if isinstance(loaded_chunked, tuple)
                else loaded_chunked
            )
            st.success("âœ… ì¸ë±ìŠ¤ ë° chunked_data ë¡œë“œ ì™„ë£Œ!")
        except FileNotFoundError:
            st.warning(f"âš ï¸ `{CHUNKED_FILE}` íŒŒì¼ì´ ì—¬ì „íˆ ì—†ìŠµë‹ˆë‹¤.")


##############################
## íƒ€ì´í‹€ ë° ì¸ì‚¬ë§ ì¶”ê°€
##############################
# íƒ€ì´í‹€ì€ ì‚¬ì´ë“œë°”ì—ë§Œ í‘œì‹œ
with st.sidebar:
    st.title("ìš©ì¸ ì‹œì²­ RAG ì±—ë´‡")

# ì‚¬ì´ë“œë°”: ëª¨ë“œ ì„ íƒ ë° ì´ˆê¸°í™” ë²„íŠ¼
with st.sidebar:
    st.title("RAG ëª¨ë“œ ì„¤ì •")

    # ì„¸ì…˜ ìƒíƒœì— ëª¨ë“œ ì €ì¥
    if "rag_mode" not in st.session_state:
        st.session_state["rag_mode"] = "base"

    # ëŒ€í™” ì‹œì‘ ì—¬ë¶€ ì¶”ì 
    if "conversation_started" not in st.session_state:
        st.session_state["conversation_started"] = False

    # ëª¨ë“œ ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼
    selected_mode = st.radio(
        "ëª¨ë“œ ì„ íƒ",
        options=list(RAG_MODES.keys()),
        format_func=lambda x: f"{RAG_MODES[x]['name']}",
        index=list(RAG_MODES.keys()).index(st.session_state["rag_mode"]),
    )

    # ëª¨ë“œê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œ
    if selected_mode != st.session_state["rag_mode"]:
        st.session_state["rag_mode"] = selected_mode
        # ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”
        if "messages" in st.session_state:
            st.session_state["messages"] = []
        # ì²´ì¸ ì´ˆê¸°í™” (ìƒˆ ëª¨ë“œì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë‹¤ì‹œ ìƒì„±)
        if "chain" in st.session_state:
            st.session_state.pop("chain")
        # ìƒˆ ëª¨ë“œë¡œ ì¸ë±ìŠ¤ ë¡œë“œ
        load_or_create_index(selected_mode)
        st.success(f"âœ… {RAG_MODES[selected_mode]['name']}ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ (ëª¨ë“œ ë³€ê²½ ì ìš©ì„ ìœ„í•´)
        st.rerun()

    # ëª¨ë¸ ì„ íƒ (ì£¼ì„ ì²˜ë¦¬ëœ ë¶€ë¶„ì€ í•„ìš”ì— ë”°ë¼ í™œì„±í™”)
    selected_model = "gpt-4o-mini"

    # ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
    with st.expander("ë””ë²„ê·¸ ì •ë³´"):
        st.write(f"í˜„ì¬ ëª¨ë“œ: {st.session_state['rag_mode']}")
        st.write(f"ë¬¸ì„œ ê°œìˆ˜: {len(db.documents)}")
        st.write(f"ì²­í¬ ê°œìˆ˜: {len(db.chunked_data.get('all_chunks', []))}")
        st.write(f"ì¸ë±ìŠ¤ í¬ê¸°: {db.index.ntotal if db.index else 0}")

        if st.button("ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ"):
            reset_db_state()
            load_or_create_index(st.session_state["rag_mode"])
            st.success("âœ… ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ ì™„ë£Œ!")
            st.rerun()

# í˜„ì¬ ëª¨ë“œë¡œ ì¸ë±ìŠ¤ ë¡œë“œ
load_or_create_index(st.session_state["rag_mode"])


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ ë° ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


def is_greeting(text: str) -> bool:
    """
    ì¸ì‚¬ë§ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜.
    ê°„ë‹¨í•œ ì¸ì‚¬ë§ì€ í™˜ì˜ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    greetings = ["ì•ˆë…•", "ì•ˆë…•?", "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì„¸ìš”?"]
    return text.strip() in greetings


# --- ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ runnable ì •ì˜ ---
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ì„ ì½ì–´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì±„ì›Œ ë„£ëŠ” ì—­í• 
def load_prompt(file_path: str, encoding: str = "utf-8") -> str:
    with open(file_path, encoding=encoding) as f:
        return f.read()


class RunnablePrompt(Runnable):
    def __init__(self, prompt_template: str):
        self.prompt_template = prompt_template

    def invoke(self, input_dict: dict, config=None, **kwargs) -> str:
        # input_dictì—ì„œ "question" í‚¤ë¥¼ ê°€ì ¸ì™€ í…œí”Œë¦¿ì— ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.
        question = input_dict.get("question", "")
        prompt_text = self.prompt_template.format(question=question)
        # ì´ì œ ë¬¸ìì—´(prompt_text)ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        return prompt_text

    # (ì¶”ê°€ ë©”ì„œë“œ êµ¬í˜„ì€ í•„ìš”ì— ë”°ë¼)


# --- ì²´ì¸ ìƒì„± ---
def create_chain(model_name="gpt-4o", mode="base"):
    # í˜„ì¬ ëª¨ë“œì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ
    prompt_file = RAG_MODES[mode]["prompt_file"]
    print(prompt_file)
    prompt_template = load_prompt(prompt_file, encoding="utf-8")
    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ runnable ìƒì„±
    prompt_runnable = RunnablePrompt(prompt_template)
    # streaming=True ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í™œì„±í™”í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name=model_name, temperature=0, streaming=True)
    # ì²´ì¸ êµ¬ì„±: ì´ˆê¸° ì…ë ¥ì€ {"question": <ì‚¬ìš©ì ì§ˆë¬¸>}ë¥¼ RunnablePassthrough()ë¡œ ê·¸ëŒ€ë¡œ ë„˜ê¸°ê³ ,
    # ê·¸ ë‹¤ìŒ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ runnableë¡œ í…œí”Œë¦¿ ì ìš©, ì´í›„ llm í˜¸ì¶œ, ë§ˆì§€ë§‰ì— StrOutputParser()ë¡œ ê²°ê³¼ íŒŒì‹±.
    chain = (
        {"question": RunnablePassthrough()} | prompt_runnable | llm | StrOutputParser()
    )
    return chain


# ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„±
def rewrite_query(user_question: str) -> str:
    """
    LLMì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìš©ì¸ì‹œì²­ ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    """
    llm_rewriter = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=False)
    rewriter_prompt = (
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìš©ì¸ì‹œì²­ì— ê´€ë ¨ëœ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ì–´ì¤˜. "
        "ì§ˆë¬¸ì˜ ì˜ë¯¸ì™€ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•´ì„œ ê²€ìƒ‰ ê²°ê³¼ì— ìµœì‹  ì†Œì‹ì´ ì˜ í¬í•¨ë  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•´ì¤˜.\n"
        f"ì§ˆë¬¸: {user_question}\n"
        "ê²€ìƒ‰ ì¿¼ë¦¬:"
    )
    rewritten = llm_rewriter.invoke(rewriter_prompt)
    if hasattr(rewritten, "content"):
        return rewritten.content.strip()
    else:
        return str(rewritten).strip()


# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    current_mode = st.session_state["rag_mode"]
    chain = create_chain(model_name="gpt-4o", mode=current_mode)
    st.session_state["chain"] = chain

# ìµœì´ˆ ì ‘ì† ì‹œ ì±—ë´‡ ì¸ì‚¬ë§ ìë™ ì¶”ê°€
if not st.session_state["messages"]:
    current_mode = st.session_state["rag_mode"]
    add_message("assistant", GREETING_MESSAGE[current_mode])

print_messages()
# ì±„íŒ… ì…ë ¥ì°½ ì¶”ê°€
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning_msg = st.empty()

if user_input:

    if st.session_state["rag_mode"] in ["article", "research", "policy", "event_doc"]:
        st.chat_message("user").write(user_input)
        # âœ… "doc" ëª¨ë“œì—ì„œë„ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©
        prompt_file = RAG_MODES[st.session_state["rag_mode"]]["prompt_file"]
        prompt_template = load_prompt(prompt_file, encoding="utf-8")

        # âœ… ì‚¬ìš©ì ì…ë ¥ì„ í”„ë¡¬í”„íŠ¸ì— ì ìš© (question ë³€ìˆ˜ë¡œ ì „ë‹¬)
        formatted_query = prompt_template.format(question=user_input)

        # âœ… GPT í˜¸ì¶œ (gpt-4o ì‚¬ìš©)
        llm = ChatOpenAI(model_name="gpt-4o", temperature=0, streaming=True)
        response_generator = llm.stream(formatted_query)

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            spinner_placeholder = st.empty()
            spinner_placeholder.markdown("**ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...**")

            for token in response_generator:
                if hasattr(token, "content"):  # âœ… AIMessageChunk ê°ì²´ì¼ ê²½ìš°
                    token_text = token.content
                else:
                    token_text = str(token)  # âœ… ë¬¸ìì—´ ë³€í™˜

                if ai_answer == "":
                    spinner_placeholder.empty()

                ai_answer += token_text
                container.markdown(ai_answer)

        log_debug(f"ìµœì¢… AI ë‹µë³€ (doc ëª¨ë“œ) = {ai_answer}")

        add_message("user", user_input)
        add_message("assistant", ai_answer)

    else:
        # âœ… "doc" ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°, ê¸°ì¡´ RAG ê²€ìƒ‰ ìˆ˜í–‰
        chain = st.session_state["chain"]
        if chain is not None:
            st.chat_message("user").write(user_input)
            try:
                query_for_search = user_input
                results = search_top_k(query_for_search, top_k=5, ranking_mode="rrf")
                log_debug(f"1ì°¨ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(results)}")

                if not results:
                    with st.spinner("ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        query_for_search = rewrite_query(user_input)
                    results = search_top_k(
                        query_for_search, top_k=3, ranking_mode="rrf"
                    )
                    log_debug(f"2ì°¨ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(results)}")
            except Exception as e:
                log_debug(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                results = []
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

            # RAG ê²°ê³¼ í‰ê°€ ë° fallback
            def get_context_text(results):
                if results and len(results) > 0:
                    summarized = summarize_sources(results)
                    if len(summarized) < 50 or "ë‚´ìš© ì—†ìŒ" in summarized:
                        return None
                    return f"ğŸ“Œ **ì¶œì²˜ ê¸°ë°˜ ì •ë³´**\n{summarized}"
                return None

            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if results and len(results) > 0:
                context_text = get_context_text(results)
                log_debug(f"ìµœì¢… context_text = {context_text}")

                answer_chunks = []
                for r in results[:3]:
                    chunk_text = r.get("chunk_text", "ë‚´ìš© ì—†ìŒ")
                    doc_url = r.get("original_doc", {}).get("url", "ì¶œì²˜ ì—†ìŒ")
                    enriched_chunk = (
                        f"ì´ chunkëŠ” {doc_url} ì—ì„œ ê°€ì ¸ì˜¨ ë‚´ìš©ì…ë‹ˆë‹¤.\n{chunk_text}"
                    )
                    answer_chunks.append(enriched_chunk)
                context_text = "\n\n".join(answer_chunks)
            else:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©
                context_text = (
                    get_context_text(results)
                    if results
                    else "ğŸ“Œ **AI ìƒì„± ë‹µë³€**\nê²€ìƒ‰ëœ ê³µì‹ ë¬¸ì„œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì•„ë˜ ë‹µë³€ì€ ìë™ ìƒì„±ëœ ê²ƒì…ë‹ˆë‹¤. "
                    "ì´ ë‹µë³€ì€ ë¶€ì •í™•í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë“œì‹œ ê³µì‹ í™ˆí˜ì´ì§€(yongin.go.kr)ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
                )

            conversation_history = ""
            if len(st.session_state["messages"]) > 1:
                recent_msgs = st.session_state["messages"][-5:]
                for msg in recent_msgs:
                    conversation_history += f"{msg.role.capitalize()}: {msg.content}\n"
                if len(conversation_history) > 500:
                    conversation_history = summarize_conversation(conversation_history)

            conversation_section = (
                f"ì´ì „ ëŒ€í™” ë‚´ìš©:\n{conversation_history}\n"
                if conversation_history
                else ""
            )
            combined_query = (
                f"ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© (RAG):\n{context_text}\n\n"
                f"{conversation_section}ìµœì¢… ì§ˆë¬¸: {user_input}"
            )

            response_generator = chain.stream(combined_query)
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                spinner_placeholder = st.empty()
                spinner_placeholder.markdown("**ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...**")
                for token in response_generator:
                    if ai_answer == "":
                        spinner_placeholder.empty()
                    ai_answer += token
                    container.markdown(ai_answer)

            log_debug(f"ìµœì¢… AI ë‹µë³€ (í•œêµ­ì–´) = {ai_answer}")

            add_message("user", user_input)
            add_message("assistant", ai_answer)

        else:
            st.error("ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
