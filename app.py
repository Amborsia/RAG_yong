import streamlit as st

from constants import GREETING_MESSAGE
from custom_logging import langsmith
from services.load_or_create_index import load_or_create_index
from services.search import search_top_k
from utils.chat import (
    add_message,
    create_chain,
    detect_language,
    get_context_text,
    is_greeting,
    print_messages,
    rewrite_query,
    summarize_sources,
    translate_text,
)
from utils.logging import log_debug

langsmith(project_name="Yong-in RAG")

INDEX_FILE = "faiss_index.bin"
CHUNKED_FILE = "chunked_data.pkl"
DATA_DIR = "data/yongin_data2"
MODELS = {
    "gpt-4o-mini": "gpt-4o-mini",
    "gpt-4o": "gpt-4o",
    "gpt-4-turbo": "gpt-4-turbo",
}

load_or_create_index()

##############################
## íƒ€ì´í‹€ ë° ì¸ì‚¬ë§ ì¶”ê°€
##############################
st.title("ìš©ì¸ ì‹œì²­ RAG ì±—ë´‡")
st.write(
    "ì•ˆë…•í•˜ì„¸ìš”! ìš©ì¸ì‹œ ê´€ë ¨ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ì•„ëž˜ ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ìž…ë ¥í•´ ì£¼ì„¸ìš”."
)

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    chain = create_chain(model_name=MODELS["gpt-4o-mini"])
    st.session_state["chain"] = chain

# ìµœì´ˆ ì ‘ì† ì‹œ ì±—ë´‡ ì¸ì‚¬ë§ ìžë™ ì¶”ê°€
if not st.session_state["messages"]:
    add_message("assistant", GREETING_MESSAGE)

selected_model = MODELS["gpt-4o-mini"]

print_messages()

# ì‚¬ìš©ìž ìž…ë ¥ ì²˜ë¦¬ (ì±— ìž…ë ¥)
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning_msg = st.empty()

if user_input:
    chain = st.session_state["chain"]
    if chain is not None:
        # ë‹¨ìˆœ ì¸ì‚¬ë§ì´ë©´ ì¸ì‚¬ë§ ì‘ë‹µ ì²˜ë¦¬
        if is_greeting(user_input):
            assistant_reply = (
                "ì•ˆë…•í•˜ì„¸ìš”! ìš©ì¸ì‹œì²­ ì±—ë´‡ìž…ë‹ˆë‹¤. ê¶ê¸ˆí•˜ì‹  ì‚¬í•­ì´ ìžˆìœ¼ì‹œë©´ íŽ¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”.\n\n"
                "ì˜ˆì‹œ ì§ˆë¬¸:\n"
                "- ìš©ì¸ì‹œì²­ ì „í™”ë²ˆí˜¸ ì•Œë ¤ì¤˜\n"
                "- ëŒ€í˜• ìƒí™œíê¸°ë¬¼ ì–´ë–»ê²Œ ë²„ë ¤?\n"
                "- ì¼ë°˜ ì“°ë ˆê¸° ë™ë³„ ë°°ì¶œì¼ ì•Œë ¤ì£¼ì„¸ìš”\n"
                "- ì—¬ê¶Œë°œê¸‰ í•„ìš”ì„œë¥˜ ë° ë°œê¸‰ê¸°ê°„ì€?\n"
                "- ìš©ì¸ì‹œ ê³µì› ì˜ˆì•½ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?\n"
                "- ìš©ì¸ì‹œ ì‹œë‚´ë²„ìŠ¤ ë…¸ì„  ì•Œë ¤ì¤˜"
            )
            st.chat_message("assistant").write(assistant_reply)
            add_message("assistant", assistant_reply)
        else:
            st.chat_message("user").write(user_input)

            # 1ì°¨ ê²€ìƒ‰: ì‚¬ìš©ìž ìž…ë ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            query_for_search = user_input
            results = search_top_k(query_for_search, top_k=3, ranking_mode="rrf")
            log_debug(f"1ì°¨ ê²€ìƒ‰ ì¿¼ë¦¬ = {query_for_search}")
            log_debug(f"1ì°¨ RAG ê²°ê³¼ = {results}")

            # 2ì°¨ ê²€ìƒ‰: ê²°ê³¼ ì—†ìœ¼ë©´ ìž¬ìž‘ì„± ì¿¼ë¦¬ ì‚¬ìš©
            if not results or len(results) == 0:
                with st.spinner("ê²€ìƒ‰ ì¿¼ë¦¬ ìž¬ìž‘ì„± ì¤‘ìž…ë‹ˆë‹¤..."):
                    query_for_search = rewrite_query(user_input)
                results = search_top_k(query_for_search, top_k=3, ranking_mode="rrf")
                log_debug(f"2ì°¨ ê²€ìƒ‰ ì¿¼ë¦¬ = {query_for_search}")
                log_debug(f"2ì°¨ RAG ê²°ê³¼ = {results}")

            # RAG ê²°ê³¼ í‰ê°€ ë° fallback
            def get_context_text(results):
                if results and len(results) > 0:
                    summarized = summarize_sources(results)
                    if len(summarized) < 50 or "ë‚´ìš© ì—†ìŒ" in summarized:
                        return None
                    return f"ðŸ“Œ **ì¶œì²˜ ê¸°ë°˜ ì •ë³´**\n{summarized}"
                return None

            context_text = get_context_text(results)
            log_debug(f"ìµœì¢… context_text = {context_text}")
            if context_text is None:
                context_text = (
                    "ðŸ“Œ **AI ìƒì„± ë‹µë³€**\nê²€ìƒ‰ëœ ê³µì‹ ë¬¸ì„œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì•„ëž˜ ë‹µë³€ì€ ìžë™ ìƒì„±ëœ ê²ƒìž…ë‹ˆë‹¤. "
                    "ì´ ë‹µë³€ì€ ë¶€ì •í™•í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ë°˜ë“œì‹œ ê³µì‹ í™ˆíŽ˜ì´ì§€(yongin.go.kr)ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
                )

            # ì´ì „ ëŒ€í™” ë‚´ì—­ì€ í¬í•¨í•˜ì§€ ì•Šê³ , ì˜¤ì§ í˜„ìž¬ ì§ˆë¬¸ë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            combined_query = (
                f"ì•„ëž˜ëŠ” ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© (RAG):\n{context_text}\n\n"
                f"ìµœì¢… ì§ˆë¬¸: {user_input}"
            )
            log_debug(f"ìµœì¢… ì¿¼ë¦¬ = {combined_query}")

            # ë‹¤êµ­ì–´ ì²˜ë¦¬: ìž…ë ¥ ì–¸ì–´ ê°ì§€ í›„ í•„ìš” ì‹œ ë²ˆì—­
            detected_lang = detect_language(user_input)
            log_debug(f"ê°ì§€ëœ ì–¸ì–´ = {detected_lang}")
            if detected_lang != "ko":
                combined_query = translate_text(combined_query, detected_lang)
                log_debug(f"ë²ˆì—­ëœ ìµœì¢… ì¿¼ë¦¬ = {combined_query}")

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            response_generator = chain.stream(combined_query)
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                spinner_placeholder = st.empty()
                spinner_placeholder.markdown("**ë‹µë³€ ìƒì„± ì¤‘ìž…ë‹ˆë‹¤...**")
                for token in response_generator:
                    if ai_answer == "":
                        spinner_placeholder.empty()
                    ai_answer += token
                    container.markdown(ai_answer)
            log_debug(f"ìµœì¢… AI ë‹µë³€ = {ai_answer}")

            # ëŒ€í™” ê¸°ë¡ ì €ìž¥
            add_message("user", user_input)
            add_message("assistant", ai_answer)
    else:
        warning_msg.error("ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
