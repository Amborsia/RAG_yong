# initialize.py

import numpy as np
import faiss
import os
import models.database as db
from models.embedding import encode_texts
from utils.chunking import (
    token_based_chunking,
    fixed_size_chunking,
    recursive_chunking,
)
import pickle

# initialize.py

def init_rag(
    data_dir="crawling/output",  # app.pyì—ì„œ DATA_DIRë¡œ ì„¤ì •í•œ ê²½ë¡œì™€ ì¼ì¹˜ì‹œí‚´
    chunk_strategy="token",
    chunk_param=500,
    index_type="HNSW",
    output_index_path="rag_index/index.faiss",  # app.pyì™€ ì¼ì¹˜
    output_chunk_path="rag_index/index.pkl"     # app.pyì™€ ì¼ì¹˜
):
    print(f"ğŸ” init_rag() í˜¸ì¶œë¨! (chunk_strategy={chunk_strategy}, chunk_param={chunk_param}, index_type={index_type})")
    # 1) ë¬¸ì„œ ë¡œë“œ
    db.load_data(data_dir)
    if not db.documents:
        raise ValueError("No documents loaded. Check the data directory.")
    print(f"Documents loaded: {len(db.documents)}")

    # 2) chunk ë¶„í• 
    all_chunks = []
    chunk_to_doc_map = []

    if chunk_strategy == "fixed":
        chunk_fn = lambda text: fixed_size_chunking(text, chunk_size=chunk_param)
    elif chunk_strategy == "recursive":
        chunk_fn = lambda text: recursive_chunking(text, max_tokens=chunk_param)
    else:  # ê¸°ë³¸ê°’ "token"
        chunk_fn = lambda text: token_based_chunking(text, max_tokens=chunk_param)

    for doc_idx, doc in enumerate(db.documents):
        # ìƒˆ êµ¬ì¡°ì—ì„œëŠ” ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ëŠ” doc["text"]
        content = doc.get("text", "")
        # ë©”íƒ€ë°ì´í„°ë¡œ URL ë“±ì´ í¬í•¨ë  ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        chunks = chunk_fn(content)
        for ch in chunks:
            all_chunks.append(ch)
            chunk_to_doc_map.append(doc_idx)

    print(f"Total chunks created: {len(all_chunks)}")

    # 3) chunk ì„ë² ë”©
    chunk_embeddings = encode_texts(all_chunks, batch_size=10)
    print(f"Generated {len(chunk_embeddings)} chunk embeddings.")
    if len(all_chunks) != len(chunk_embeddings):
        print(f"[Warning] Mismatch: {len(all_chunks)} chunks, {len(chunk_embeddings)} embeddings.")

    # 4) ì¸ë±ìŠ¤ ìƒì„±
    index = db.build_index(chunk_embeddings, index_type=index_type)
    if index is None or index.ntotal == 0:
        raise ValueError("FAISS index creation failed or is empty.")
    print(f"FAISS index built with {index.ntotal} chunk embeddings.")

    # 5) ì¸ë±ìŠ¤, chunk ë°ì´í„° íŒŒì¼ë¡œ ì €ì¥
    # í´ë” "rag_index"ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists("rag_index"):
        os.makedirs("rag_index")
    faiss.write_index(index, output_index_path)
    print(f"FAISS index saved to {output_index_path}.")

    db.chunked_data = {
        "all_chunks": all_chunks,
        "chunk_to_doc_map": chunk_to_doc_map,
    }

    with open(output_chunk_path, "wb") as f:
        pickle.dump(db.chunked_data, f)
    print(f"chunked_data saved to {output_chunk_path}")



def main():
    """
    ê¸°ì¡´ì²˜ëŸ¼ ë‹¨ë… ì‹¤í–‰í•  ë•Œ,
    ì¸ìë¥¼ ì§ì ‘ ë°”ê¿”ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë¶€ë¶„ ìˆ˜ì •
    ì˜ˆ) CLI ì¸ì íŒŒì‹±, sys.argv, argparse ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
    """
    init_rag(
        data_dir="data/yongin_data2",
        chunk_strategy="token",  # "fixed", "recursive", "token"
        chunk_param=500,
        index_type="HNSW",       # "FLAT" or "HNSW"
        output_index_path="faiss_index.bin",
        output_chunk_path="chunked_data.pkl"
    )

if __name__ == "__main__":
    main()
