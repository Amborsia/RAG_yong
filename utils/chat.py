import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

from utils.custom_logging import gemma_trace
from utils.prompts import load_prompt

MODELS = {
    "gpt-4o-mini": "gpt-4o-mini",
    "gpt-4o": "gpt-4o",
    "gpt-4-turbo": "gpt-4-turbo",
}


def print_messages():
    """ì„¸ì…˜ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


def add_message(role, message):
    """ì„¸ì…˜ì— ìƒˆ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


def is_greeting(text: str) -> bool:
    """
    ë‹¨ìˆœ ì¸ì‚¬ë§(ì˜ˆ: "ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”")ë§Œì„ ì¸ì‹í•˜ë„ë¡ ê°œì„ í•©ë‹ˆë‹¤.
    """
    return text.strip() in {"ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”"}


def filter_conversation(history_msgs):
    """
    ëŒ€í™” ë‚´ì—­ì—ì„œ ì¸ì‚¬ë§, TIP, ì˜ˆì‹œ ì§ˆë¬¸ ë“± ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    filtered = []
    exclusion_keywords = {
        "ì•ˆë…•",
        "ì•ˆë…•í•˜ì„¸ìš”",
        "TIP!",
        "ì˜ˆì‹œ ì§ˆë¬¸",
        "ë” ë‚˜ì€ ì‚¶ì„ ìœ„í•œ ìŠ¤ë§ˆíŠ¸ë„ì‹œ",
    }
    for msg in history_msgs:
        if not any(msg.content.startswith(keyword) for keyword in exclusion_keywords):
            filtered.append(msg)
    return filtered


def create_chain(model_name=MODELS["gpt-4o-mini"]):
    """
    í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•œ í›„, ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    prompt = load_prompt("prompts/yongin.yaml")
    llm = ChatOpenAI(model_name=model_name, temperature=0.8, streaming=True)
    chain = {"question": RunnablePassthrough()} | prompt | llm | StrOutputParser()
    return chain


def rewrite_query(user_question: str) -> str:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ìš©ì¸ì‹œì²­ ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    llm_rewriter = ChatOpenAI(
        model_name=MODELS["gpt-4o-mini"], temperature=0.8, streaming=False
    )
    rewriter_prompt = (
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìš©ì¸ì‹œì²­ ê´€ë ¨ ìµœì‹  ì •ë³´(ì •ì±…, ì„œë¹„ìŠ¤, í–‰ì‚¬ ë“±)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” "
        "í•µì‹¬ í‚¤ì›Œë“œì™€ ë¬¸ì¥ì„ í¬í•¨í•œ ìµœì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ë‹µë³€ì€ ê°„ê²°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        f"ì§ˆë¬¸: {user_question}\n"
        "ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬:"
    )
    rewritten = llm_rewriter.invoke(rewriter_prompt)
    return (
        rewritten.content.strip()
        if hasattr(rewritten, "content")
        else str(rewritten).strip()
    )


def summarize_conversation(history_text: str) -> str:
    """
    ì£¼ì–´ì§„ ëŒ€í™” ë‚´ì—­ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    llm_summary = ChatOpenAI(
        model_name=MODELS["gpt-4o-mini"], temperature=0, streaming=False
    )
    summary_prompt = (
        f"ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜:\n\n{history_text}\n\nê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜."
    )
    summary_response = llm_summary.invoke(summary_prompt)
    if hasattr(summary_response, "content"):
        return summary_response.content.strip()
    else:
        return str(summary_response).strip()


def detect_language(text: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ì „ì²´ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, í•´ë‹¹ ì–¸ì–´ì˜ ISO 639-1 ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: í•œêµ­ì–´ 'ko', ì˜ì–´ 'en', ì¼ë³¸ì–´ 'ja'
    """
    llm = ChatOpenAI(model_name=MODELS["gpt-4o-mini"], temperature=0, streaming=False)
    prompt = (
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì „ì²´ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬, í•´ë‹¹ í…ìŠ¤íŠ¸ê°€ ì–´ë–¤ ì–¸ì–´ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ISO 639-1 ì½”ë“œë¡œ í•œ ë‹¨ì–´ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”. "
        "ì˜ˆì‹œ: 'ko' (í•œêµ­ì–´), 'en' (ì˜ì–´), 'ja' (ì¼ë³¸ì–´).\n"
        f"í…ìŠ¤íŠ¸: '''{text}'''"
    )
    response = llm.invoke(prompt)
    lang = response.content.strip() if hasattr(response, "content") else "ko"
    if lang not in {"ko", "en", "ja"}:
        lang = "ko"
    return lang


def translate_text(text: str, target_lang: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ target_lang ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
    """
    llm = ChatOpenAI(model_name=MODELS["gpt-4o-mini"], temperature=0, streaming=False)
    prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ '{target_lang}'ë¡œ ë²ˆì—­í•´ì¤˜:\n{text}"
    response = llm.invoke(prompt)
    return response.content.strip() if hasattr(response, "content") else text


def summarize_sources(results):
    """
    ê²€ìƒ‰ëœ ê²°ê³¼ ì¤‘ ìµœëŒ€ 3ê°œì˜ ì¶œì²˜ ê¸°ë°˜ ì •ë³´ë¥¼ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    summarized_text = []
    for r in results[:3]:
        chunk_text = r.get("chunk_text", "ë‚´ìš© ì—†ìŒ")
        doc_url = r.get("original_doc", {}).get("url", "ì¶œì²˜ ì—†ìŒ")
        summarized_text.append(f"- {chunk_text} (ì¶œì²˜: {doc_url})")
    return "\n".join(summarized_text)


def get_context_text(results):
    """
    ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•˜ì—¬,
    ì¶©ë¶„í•˜ë©´ ì¶œì²˜ ê¸°ë°˜ ì •ë³´ë¥¼ ë°˜í™˜í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if results and len(results) > 0:
        summarized = summarize_sources(results)
        # ì˜ˆì‹œ: ìš”ì•½ëœ ê²°ê³¼ê°€ 50ì ë¯¸ë§Œì´ê±°ë‚˜ "ë‚´ìš© ì—†ìŒ"ì´ í¬í•¨ë˜ë©´ ë¶€ì¡±í•˜ë‹¤ê³  íŒë‹¨
        if len(summarized) < 50 or "ë‚´ìš© ì—†ìŒ" in summarized:
            return None
        return f"ğŸ“Œ **ì¶œì²˜ ê¸°ë°˜ ì •ë³´**\n{summarized}"
    return None
